{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”· PART 2: Predictive Data Modeling ðŸ”·"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we analyze the processed data through a **predictive** lens: we train and test segmented datasets on various machine learning models (and potentially advanced machine learning and/or deep learning algorithms) to attain a well-performing predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”µ TABLE OF CONTENTS ðŸ”µ <a name=\"TOC\"></a>\n",
    "\n",
    "Use this **table of contents** to navigate the various sections of the predictive data modeling notebook.\n",
    "\n",
    "#### 1. [Section A: Imports and Initializations](#section-A)\n",
    "\n",
    "    All necessary imports and object instantiations for predictive analytics.\n",
    "\n",
    "#### 2. [Section B: Data Processing & Finalization](#section-B)\n",
    "\n",
    "    Data curation and preparation for directed predictive modeling.\n",
    "\n",
    "#### 6. [Section C: Introductory Machine Learning](#section-D)\n",
    "\n",
    "    Use of classical and basic machine learning algorithms to run predictive modeling.\n",
    "\n",
    "#### 7. [Section D: Deep Learning & Advanced Modeling](#section-E)\n",
    "\n",
    "    Use of deep learning and advanced statistical algorithms to run predictive modeling.\n",
    "    \n",
    "#### 8. [Appendix: Supplementary Custom Objects](#appendix)\n",
    "\n",
    "    Custom Python object architectures used throughout the data predictions.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section A: Imports and Initializations <a name=\"section-A\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Imports for Data Manipulation and Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Algorithmic Structures for Processed Data Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../source/structures\")\n",
    "\n",
    "# TODO: Place custom structures from `../source/structures` here.\n",
    "sys.path.insert(0, os.path.abspath('../helper'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section B: Data Processing & Finalization <a name=\"section-B\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Function for Bilingual Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function import load_clean\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import load\n",
    "\n",
    "def load_clean(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = load(f)\n",
    "    return data\n",
    "\n",
    "#tokenize text\n",
    "def tokenize_words(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_len(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "data = load_clean('../datasets/processed/eng-fra-both.pickle')\n",
    "train = load_clean('../datasets/processed/eng-fra-train.pickle')\n",
    "test = load_clean('../datasets/processed/eng-fra-test.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary size: 2912\n",
      "Max Length of English Vocab: 5\n"
     ]
    }
   ],
   "source": [
    "engl_tokens = tokenize_words(data[:, 0])\n",
    "eng_vocab_size = len(engl_tokens.word_index) + 1\n",
    "eng_len = max_len(data[:, 0])\n",
    "\n",
    "print(\"English Vocabulary size: {}\".format(eng_vocab_size))\n",
    "print(\"Max Length of English Vocab: {}\".format(eng_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Vocabulary size: 5791\n",
      "Max Length of French Vocab: 10\n"
     ]
    }
   ],
   "source": [
    "fra_tokens = tokenize_words(data[:, 1])\n",
    "fra_vocab_size = len(fra_tokens.word_index) + 1\n",
    "fra_len = max_len(data[:, 1])\n",
    "\n",
    "print(\"French Vocabulary size: {}\".format(fra_vocab_size))\n",
    "print(\"Max Length of French Vocab: {}\".format(fra_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Input and Output to Ints/Pad to Max Phrase Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(tokenizer, length, lines):\n",
    "    #integer encoding input\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    #padding sequences with 0 to max length\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(sequences, vocab_size):\n",
    "    y_list = []\n",
    "    for s in sequences:\n",
    "        encoded = to_categorical(s, num_classes=vocab_size)\n",
    "        y_list.append(encoded)\n",
    "    \n",
    "    y = array(y_list)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encode_input(fra_tokens, fra_len, train[:, 1])\n",
    "Y_train = encode_input(engl_tokens, eng_len, train[:, 0])\n",
    "Y_train = encode_output(Y_train, eng_vocab_size)\n",
    "\n",
    "X_test = encode_input(fra_tokens, fra_len, test[:, 1])\n",
    "Y_test = encode_input(engl_tokens, eng_len, test[:, 0])\n",
    "Y_test = encode_output(Y_test, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section C: Introductory Machine Learning <a name=\"section-C\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section D: Deep Learning & Advanced Modeling <a name=\"section-D\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Appendix: Supplementary Custom Objects <a name=\"appendix\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
