{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”· PART 2: Predictive Data Modeling ðŸ”·"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we analyze the processed data through a **predictive** lens: we train and test segmented datasets on various machine learning models (and potentially advanced machine learning and/or deep learning algorithms) to attain a well-performing predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”µ TABLE OF CONTENTS ðŸ”µ <a name=\"TOC\"></a>\n",
    "\n",
    "Use this **table of contents** to navigate the various sections of the predictive data modeling notebook.\n",
    "\n",
    "#### 1. [Section A: Imports and Initializations](#section-A)\n",
    "\n",
    "    All necessary imports and object instantiations for predictive analytics.\n",
    "\n",
    "#### 2. [Section B: Data Processing & Finalization](#section-B)\n",
    "\n",
    "    Data curation and preparation for directed predictive modeling.\n",
    "\n",
    "#### 6. [Section C: Encoder-Decoder LSTM Model](#section-D)\n",
    "\n",
    "    Use of deep learning model to translate between different languages.\n",
    "\n",
    "#### 7. [Section D: Evaluating Model](#section-E)\n",
    "\n",
    "    Evaluating how well the model performs.\n",
    "    \n",
    "#### 8. [Appendix: Supplementary Custom Objects](#appendix)\n",
    "\n",
    "    Custom Python object architectures used throughout the data predictions.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section A: Imports and Initializations <a name=\"section-A\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Imports for Data Manipulation and Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/makeschoolloaner/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Algorithmic Structures for Processed Data Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../source/structures\")\n",
    "\n",
    "# TODO: Place custom structures from `../source/structures` here.\n",
    "sys.path.insert(0, os.path.abspath('../helper'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section B: Data Processing & Finalization <a name=\"section-B\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Function for Bilingual Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function import load_clean\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import load\n",
    "\n",
    "def load_clean(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = load(f)\n",
    "    return data\n",
    "\n",
    "#tokenize text\n",
    "def tokenize_words(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_len(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "data = load_clean('../datasets/processed/eng-fra-both.pickle')\n",
    "train = load_clean('../datasets/processed/eng-fra-train.pickle')\n",
    "test = load_clean('../datasets/processed/eng-fra-test.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary size: 2912\n",
      "Max Length of English Vocab: 5\n"
     ]
    }
   ],
   "source": [
    "engl_tokens = tokenize_words(data[:, 0])\n",
    "eng_vocab_size = len(engl_tokens.word_index) + 1\n",
    "eng_len = max_len(data[:, 0])\n",
    "\n",
    "print(\"English Vocabulary size: {}\".format(eng_vocab_size))\n",
    "print(\"Max Length of English Vocab: {}\".format(eng_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Vocabulary size: 5791\n",
      "Max Length of French Vocab: 10\n",
      "Shape of Input Vector: 5791x10\n"
     ]
    }
   ],
   "source": [
    "fra_tokens = tokenize_words(data[:, 1])\n",
    "fra_vocab_size = len(fra_tokens.word_index) + 1\n",
    "fra_len = max_len(data[:, 1])\n",
    "\n",
    "print(\"French Vocabulary size: {}\".format(fra_vocab_size))\n",
    "print(\"Max Length of French Vocab: {}\".format(fra_len))\n",
    "print(\"Shape of Input Vector: {}x{}\".format(fra_vocab_size, fra_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Input and Output to Ints/Pad to Max Phrase Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(tokenizer, length, lines):\n",
    "    #integer encoding input\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    #padding sequences with 0 to max length\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(sequences, vocab_size):\n",
    "    y_list = []\n",
    "    for s in sequences:\n",
    "        encoded = to_categorical(s, num_classes=vocab_size)\n",
    "        y_list.append(encoded)\n",
    "    \n",
    "    y = array(y_list)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encode_input(fra_tokens, fra_len, train[:, 1])\n",
    "Y_train = encode_input(engl_tokens, eng_len, train[:, 0])\n",
    "Y_train = encode_output(Y_train, eng_vocab_size)\n",
    "\n",
    "X_test = encode_input(fra_tokens, fra_len, test[:, 1])\n",
    "Y_test = encode_input(engl_tokens, eng_len, test[:, 0])\n",
    "Y_test = encode_output(Y_test, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy Function to Convert Logits to Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this function is to be able to see the output of the Neural Network. Credit to Tommy Tracey for code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section C: Encoder-Decoder LSTM Model\n",
    "<a name=\"section-C\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_vocab, output_vocab, input_time_steps, targ_time_steps, n_units):\n",
    "    \"\"\"\n",
    "    Model Explanation here\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_vocab, n_units, input_length=input_time_steps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(targ_time_steps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(output_vocab, activation='softmax')))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 200)           1158200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 200)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 200)            320800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 2912)           585312    \n",
      "=================================================================\n",
      "Total params: 2,385,112\n",
      "Trainable params: 2,385,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model(fra_vocab_size, eng_vocab_size, fra_len, eng_len, 200)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "#summarize model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/30\n",
      " - 39s - loss: 4.3869 - val_loss: 4.5758\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.57584, saving model to model.h5\n",
      "Epoch 2/30\n",
      " - 36s - loss: 3.4302 - val_loss: 4.4892\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.57584 to 4.48918, saving model to model.h5\n",
      "Epoch 3/30\n",
      " - 40s - loss: 3.2662 - val_loss: 4.4930\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.48918\n",
      "Epoch 4/30\n",
      " - 39s - loss: 3.1116 - val_loss: 4.4478\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.48918 to 4.44775, saving model to model.h5\n",
      "Epoch 5/30\n",
      " - 36s - loss: 2.9405 - val_loss: 4.2921\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.44775 to 4.29206, saving model to model.h5\n",
      "Epoch 6/30\n",
      " - 36s - loss: 2.7860 - val_loss: 4.2787\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.29206 to 4.27866, saving model to model.h5\n",
      "Epoch 7/30\n",
      " - 35s - loss: 2.6469 - val_loss: 4.1691\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.27866 to 4.16909, saving model to model.h5\n",
      "Epoch 8/30\n",
      " - 36s - loss: 2.5137 - val_loss: 4.1833\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.16909\n",
      "Epoch 9/30\n",
      " - 35s - loss: 2.3830 - val_loss: 4.0769\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.16909 to 4.07693, saving model to model.h5\n",
      "Epoch 10/30\n",
      " - 36s - loss: 2.2483 - val_loss: 4.0423\n",
      "\n",
      "Epoch 00010: val_loss improved from 4.07693 to 4.04226, saving model to model.h5\n",
      "Epoch 11/30\n",
      " - 39s - loss: 2.1250 - val_loss: 4.0752\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 4.04226\n",
      "Epoch 12/30\n",
      " - 38s - loss: 2.0013 - val_loss: 4.0347\n",
      "\n",
      "Epoch 00012: val_loss improved from 4.04226 to 4.03469, saving model to model.h5\n",
      "Epoch 13/30\n",
      " - 41s - loss: 1.8861 - val_loss: 4.0054\n",
      "\n",
      "Epoch 00013: val_loss improved from 4.03469 to 4.00536, saving model to model.h5\n",
      "Epoch 14/30\n",
      " - 44s - loss: 1.7739 - val_loss: 3.9676\n",
      "\n",
      "Epoch 00014: val_loss improved from 4.00536 to 3.96759, saving model to model.h5\n",
      "Epoch 15/30\n",
      " - 40s - loss: 1.6666 - val_loss: 3.9760\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.96759\n",
      "Epoch 16/30\n",
      " - 38s - loss: 1.5642 - val_loss: 3.9429\n",
      "\n",
      "Epoch 00016: val_loss improved from 3.96759 to 3.94292, saving model to model.h5\n",
      "Epoch 17/30\n",
      " - 37s - loss: 1.4636 - val_loss: 3.8890\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.94292 to 3.88896, saving model to model.h5\n",
      "Epoch 18/30\n",
      " - 37s - loss: 1.3683 - val_loss: 3.9234\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3.88896\n",
      "Epoch 19/30\n",
      " - 36s - loss: 1.2770 - val_loss: 3.9499\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.88896\n",
      "Epoch 20/30\n",
      " - 37s - loss: 1.1902 - val_loss: 3.8960\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.88896\n",
      "Epoch 21/30\n",
      " - 53s - loss: 1.1051 - val_loss: 3.9258\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 3.88896\n",
      "Epoch 22/30\n",
      " - 44s - loss: 1.0280 - val_loss: 3.9179\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 3.88896\n",
      "Epoch 23/30\n",
      " - 50s - loss: 0.9517 - val_loss: 3.9376\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 3.88896\n",
      "Epoch 24/30\n",
      " - 44s - loss: 0.8792 - val_loss: 3.9653\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 3.88896\n",
      "Epoch 25/30\n",
      " - 43s - loss: 0.8144 - val_loss: 3.9990\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 3.88896\n",
      "Epoch 26/30\n",
      " - 42s - loss: 0.7530 - val_loss: 3.9634\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 3.88896\n",
      "Epoch 27/30\n",
      " - 40s - loss: 0.6943 - val_loss: 3.9269\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 3.88896\n",
      "Epoch 28/30\n",
      " - 40s - loss: 0.6438 - val_loss: 3.9954\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 3.88896\n",
      "Epoch 29/30\n",
      " - 42s - loss: 0.5929 - val_loss: 4.0520\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 3.88896\n",
      "Epoch 30/30\n",
      " - 43s - loss: 0.5481 - val_loss: 4.0666\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 3.88896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a622a04a8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(X_train, Y_train, epochs=30, batch_size=64, validation_data=(X_test, Y_test), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section D: Model Evaluation <a name=\"section-D\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5.3949616e-06 3.9550187e-06 8.8687672e-04 ... 1.9808064e-09\n",
      "   1.9636830e-09 1.2631832e-09]\n",
      "  [6.4501633e-05 1.2913855e-05 7.5000986e-05 ... 2.6966077e-08\n",
      "   2.3724809e-08 1.7406707e-08]\n",
      "  [5.5918777e-01 7.4381511e-08 1.9888962e-03 ... 2.1655049e-08\n",
      "   1.5560998e-08 1.0444708e-08]\n",
      "  [9.9992716e-01 1.3637837e-10 7.3377052e-08 ... 4.9240564e-12\n",
      "   2.4985623e-12 1.8299058e-12]\n",
      "  [9.9997509e-01 4.8153457e-11 7.1639668e-09 ... 1.3429669e-12\n",
      "   6.2668073e-13 4.2473520e-13]]\n",
      "\n",
      " [[2.2723200e-06 5.7536447e-08 9.4584556e-07 ... 8.0793694e-10\n",
      "   7.0415640e-10 6.2333538e-10]\n",
      "  [9.1253500e-03 1.5286450e-06 1.3363203e-07 ... 1.1299251e-07\n",
      "   8.9393730e-08 9.3803436e-08]\n",
      "  [9.9759299e-01 4.2207129e-10 1.4012790e-09 ... 2.5091748e-10\n",
      "   1.7342135e-10 1.5353793e-10]\n",
      "  [9.9992812e-01 2.7022638e-11 7.7491513e-11 ... 4.7471250e-12\n",
      "   2.8146179e-12 2.6336899e-12]\n",
      "  [9.9995482e-01 1.7271412e-11 1.3468428e-10 ... 1.5378096e-12\n",
      "   9.3945804e-13 8.3484169e-13]]\n",
      "\n",
      " [[3.7028822e-06 4.6479195e-06 1.6592587e-05 ... 3.3884462e-09\n",
      "   2.7662497e-09 1.9378430e-09]\n",
      "  [3.7757152e-05 3.8094935e-05 6.2944318e-06 ... 6.4826944e-09\n",
      "   4.9677338e-09 4.3460617e-09]\n",
      "  [8.5179936e-03 1.5381056e-05 8.1306798e-06 ... 8.9451415e-09\n",
      "   4.8073887e-09 4.2744346e-09]\n",
      "  [9.5030677e-01 5.0272124e-08 6.0381922e-08 ... 2.7151789e-10\n",
      "   1.6624201e-10 1.3184956e-10]\n",
      "  [9.9964857e-01 3.9046100e-10 2.6786465e-10 ... 3.6893821e-12\n",
      "   2.0149318e-12 1.5599854e-12]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[4.0924938e-08 1.9175600e-07 6.7807971e-03 ... 8.4603401e-11\n",
      "   9.7051568e-11 4.4753680e-11]\n",
      "  [2.8681186e-06 3.3328426e-08 5.6169927e-04 ... 7.5915860e-09\n",
      "   6.5720633e-09 3.6106436e-09]\n",
      "  [3.5834227e-02 5.1527227e-09 1.2703146e-03 ... 1.8848478e-09\n",
      "   1.3417397e-09 6.2674488e-10]\n",
      "  [9.9978036e-01 7.3181683e-11 3.6432102e-08 ... 1.5342925e-12\n",
      "   9.4790603e-13 4.7433986e-13]\n",
      "  [9.9990463e-01 2.2506786e-11 5.6324545e-09 ... 5.5136288e-13\n",
      "   3.3318462e-13 1.5832585e-13]]\n",
      "\n",
      " [[1.8339600e-07 8.9713460e-08 3.9732209e-04 ... 2.7433092e-10\n",
      "   2.8375749e-10 1.7616654e-10]\n",
      "  [2.4777872e-04 3.3271781e-09 8.9762698e-06 ... 8.9819077e-09\n",
      "   7.7347702e-09 5.4621712e-09]\n",
      "  [9.9474806e-01 1.8324167e-10 2.4080822e-07 ... 5.1847516e-11\n",
      "   3.2069916e-11 1.9372302e-11]\n",
      "  [9.9988067e-01 3.2417773e-11 1.3882779e-09 ... 1.8279545e-12\n",
      "   1.0241601e-12 6.8564116e-13]\n",
      "  [9.9994731e-01 3.2776386e-11 6.2336686e-10 ... 7.5457934e-13\n",
      "   4.0771770e-13 2.7742907e-13]]\n",
      "\n",
      " [[1.6886115e-07 1.0316410e-07 3.9862623e-04 ... 2.4259453e-10\n",
      "   2.5196312e-10 1.5542967e-10]\n",
      "  [2.0409717e-04 3.6425649e-09 8.1932985e-06 ... 8.6177785e-09\n",
      "   7.5135365e-09 5.2993547e-09]\n",
      "  [9.9428624e-01 1.9472762e-10 2.4762744e-07 ... 5.4002993e-11\n",
      "   3.3650714e-11 2.0219243e-11]\n",
      "  [9.9988103e-01 3.3009536e-11 1.4232703e-09 ... 1.7835785e-12\n",
      "   9.9972623e-13 6.6586846e-13]\n",
      "  [9.9994862e-01 3.3179300e-11 6.4114319e-10 ... 7.2931380e-13\n",
      "   3.9423757e-13 2.6689084e-13]]]\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(model.predict(X_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Appendix: Supplementary Custom Objects <a name=\"appendix\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see helper folder for custom python functions used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
